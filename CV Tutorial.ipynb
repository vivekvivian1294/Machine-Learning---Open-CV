{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to show you some of the practical basics of computer vision, and to allow you to get your hands dirty. You will learn the following:\n",
    "\n",
    "- How to get image and video data in and out of python\n",
    "- How to do basic image manipulation\n",
    "- How to perform color thresholding to detect the position of objects\n",
    "- How to perform face detection using a Haar-Cascade classifier\n",
    "\n",
    "We will be focussing on offline processing, i.e., we will record a video with your webcam, store it, and then process it, instead of focussing on online processing, i.e., augmenting an image in real-time. Both offline, and online processing use the same techniques - which we will introduce here; online processig just has an added constraint that your code needs to be fast as well.\n",
    "\n",
    "For each topic, you will first get a small demo introducing the concepts, and then you will solve exercises using what you have learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, import the libraries you will use throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# For Visualization in Jupyter\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Get images and video into Jupyter from your webcam\n",
    "from ipywebrtc import CameraStream, ImageRecorder, VideoRecorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get image and video data in and out of python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an Image from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import image\n",
    "path_to_file = \"logo.png\"\n",
    "image = cv2.imread(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Display\" an Image using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(image))\n",
    "print(\"Image Shape: {}\".format(image.shape))\n",
    "print(\"Image dType: {}\".format(image.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, in python an image is nothing but a numpy array. This is very powerfull as it enables a variety of image manipulations, as you will see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display an Image in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image = cv2.imencode('.png', image)[1].tostring()\n",
    "widgets.Image(value=display_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this code, you are converting the image from a numpy array into a byte string. This string represents a .png image, which you then pass into a Jupyter image widget as input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display an Image Using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened here? The image in matplotlib doesn't look like the original image or the one we displayed in the image widget above; the colors are mixed up.\n",
    "\n",
    "The reason for this is, because OpenCV and matplotlib expect different image formats for the image. As you can see in above section, an image in python is just a numpy array. The first channel corresponds to the vertical axis, the second channel to the horizontal axis, and the third axis corresponds to the color channels.\n",
    "\n",
    "In OpenCV the color channels are, by default, ordered Blue-Green-Red, whereas in matplotlib - and the majority of other image editing tools - colors are ordered Red-Blue-Green. In both cases a single color has up to 8-bit, which is why these are called BGR8, and RGB8 respectively. Further down in this tutorial you will encounter two additional formats that are important.\n",
    "\n",
    "If the order of the color channels gets mixed up, the colors of the resulting image look funny. This is what is happening above, so let's correct it using one of openCVs many other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image2 = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image from a Webcam in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get a handle to the camera. For this to work, you will have to allow this website to use your webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CameraStream(constraints=\n",
    "                      {'facing_mode': 'user',\n",
    "                       'audio': False,\n",
    "                       'video': { 'width': 640, 'height': 480 }\n",
    "                       })\n",
    "camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder = ImageRecorder(stream=camera)\n",
    "recorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are happy with the snapshot, make sure to close the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A snapshot is a byte string in .png format (by default). Therefore, after you have taken a snapshot, you need to convert it to a numpy array for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the value from the ImageRecorder\n",
    "snapshot = recorder.image.value\n",
    "snapshot = np.frombuffer(snapshot, dtype=np.uint8)\n",
    "snapshot = cv2.imdecode(snapshot, cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure everything works as expected by visualizing the extracted image (`snapshot`) using what you learned above. You can choose to display the image using either an image widget or matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Place code here to display the image\n",
    "display_image = cv2.imencode('.png', snapshot)[1].tostring()\n",
    "widgets.Image(value=display_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Image to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line below stores the image as a .png file. It is also possible to save .jpg files, by changing the filename. OpenCV will pick it up automatically and try to match the name to a supported file type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('snapshot.png', snapshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Video from Webcam in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CameraStream(constraints=\n",
    "                      {'facing_mode': 'user',\n",
    "                       'audio': False,\n",
    "                       'video': { 'width': 640, 'height': 480 }\n",
    "                       })\n",
    "camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder2 = VideoRecorder(stream=camera)\n",
    "recorder2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time you are not using an `ImageRecorder`, but a `VideoRecorder`. It also returns a byte string; however, this time it is returned in the format `.webm`.\n",
    "\n",
    "Getting videos into python is a little bit more involved. You have to first save the encoded video as a file and then load it it again using OpenCV. OpenCV will then take care of decoding the video for you so that you can access it frame by frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the file to disk\n",
    "with open('capture.webm', 'wb') as out_file:\n",
    "    out_file.write(recorder2.video.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the video file using OpenCV and display each frame using widgets\n",
    "disp = widgets.Image()\n",
    "display(disp)\n",
    "\n",
    "video_reader = cv2.VideoCapture(\"capture.webm\")\n",
    "ret, frame = video_reader.read()\n",
    "while ret:\n",
    "    ret, frame = video_reader.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    display_image = cv2.imencode('.png', frame)[1].tostring()\n",
    "    disp.value = display_image\n",
    "video_reader.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, when you run the snippet above, the video is stuttering. Some frames are displayed in real time, whereas at other times it freezes and then skips a few frames.\n",
    "\n",
    "This is because image widgets aren't built to display video. The code above feeds each frame as fast as possible rather then ensuring a consistent frame rate. We will address this in the next section, when we cover saving video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a video file in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "widgets.Video.from_file(\"capture.webm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Video from Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the image widget is not meant to display video. Instead, you can open the image, process it, and write it back to disk (using a suitable video encoding). Once it is stored again, you can view it using a video widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = cv2.VideoCapture(\"capture.webm\")\n",
    "\n",
    "output_file_name = \"output.mp4\"\n",
    "backend = cv2.CAP_ANY\n",
    "fourcc_code = cv2.VideoWriter_fourcc(*\"H264\")\n",
    "fps = 24\n",
    "frame_size = (640, 480)\n",
    "output_video = cv2.VideoWriter(output_file_name, backend, fourcc_code, fps, frame_size)\n",
    "\n",
    "ret, frame = input_video.read()\n",
    "while ret:\n",
    "    ret, frame = input_video.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    output_video.write(frame)\n",
    "input_video.release()\n",
    "output_video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If you are having issues writing .mp4 files, you can change the file type to .webm. To do this replace the second line with* `output_video = cv2.VideoWriter(\"output.webm\", cv2.CAP_ANY, cv2.VideoWriter_fourcc(*\"VP80\"), 24, (640,480))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the created file using a video widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.Video.from_file(\"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the video you created above, split it frame by frame, and save the frames to disk. \n",
    "\n",
    "Note: Since a video will have a lot of frames, it is advisable to store them in a sub-folder. You can create one using jupyters line magic. For example to create the directory `imgs` you would do: `!mkdir imgs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = cv2.VideoCapture(\"capture.webm\")\n",
    "ret, frame = foo.read()\n",
    "path = \"img/img_{}.png\"\n",
    "\n",
    "counter = 0\n",
    "while ret:\n",
    "    ret, frame = foo.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    location = path.format(counter)\n",
    "    cv2.imwrite(path.format(counter), frame)\n",
    "    counter += 1\n",
    "foo.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the batch of images you created in Exercise 3, convert the images back to a video. Afterwards, inspect the created video and verify that it matches the original input video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = cv2.VideoWriter(\"exercise4.mp4\", cv2.CAP_ANY, cv2.VideoWriter_fourcc(*\"H264\"), 24, (640,480))\n",
    "path = \"img/img_{}.png\"\n",
    "\n",
    "for counter in range(44):\n",
    "    location = path.format(counter)\n",
    "    \n",
    "    img = cv2.imread(location)\n",
    "    writer.write(img)\n",
    "\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to perform basic image manipulation in OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already know about RGB and BGR from above (displaying images with matplotlib); here, you will learn how to convert images into grayscale and what the HSV color space is about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these conversions, you will have to use the snapshot you took at the very beginning of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image = cv2.imencode('.png', snapshot)[1].tostring()\n",
    "widgets.Image(value=display_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert an BGR image to Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversion\n",
    "snapshot_gray = cv2.cvtColor(snapshot, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# display the image\n",
    "display_image = cv2.imencode('.png', snapshot_gray)[1].tostring()\n",
    "widgets.Image(value=display_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to notice here is that the gray color space only has a single channel (intensity). The resulting image is a rank 2 tensor, i.e., a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image Shape: {}\".format(snapshot_gray.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert BGR into HSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to HSV\n",
    "snapshot_hsv = cv2.cvtColor(snapshot, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "#display the image\n",
    "display_image = cv2.imencode('.png', snapshot_hsv)[1].tostring()\n",
    "widgets.Image(value=display_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HSV stands for Hue, Saturation, Value and is a [very interesting color space](https://en.wikipedia.org/wiki/HSL_and_HSV). It is inspired by how humans perceive color when mixing paint. It is very handy for color thresholding, because it projects variations in color onto one axis, while projecting changes in illumination onto the other two. This allows for a more robust color segmentation. You will do color thresholding further down in this tutorial.\n",
    "\n",
    "The reason the image looks funny is because we again violate assumptions. `imencode` converts a matrix to a byte string, and assumes that the matrix is an image in BGR format; however, we feed it an image in HSV format. Thus we can see some interesting colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the gray image into a rank 3 tensor to match the format of the colored images\n",
    "snapshot_gray_img = np.stack([snapshot_gray] * 3, axis = 2)\n",
    "\n",
    "# stack up the images - like you would stack matrices\n",
    "stacked = np.hstack([snapshot_hsv, snapshot, snapshot_gray_img])\n",
    "\n",
    "# display the new image\n",
    "display_image = cv2.imencode('.png', stacked)[1].tostring()\n",
    "widgets.Image(value=display_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Text on an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to HSV\n",
    "snapshot_hsv = cv2.cvtColor(snapshot, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# display a text on the image\n",
    "text_string = \"HSV\"\n",
    "position = (5, 50) #of bottom left corner of the text\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_size = 2\n",
    "font_color = (255, 255, 255) # Remember its BGR\n",
    "font_thickness = 2\n",
    "cv2.putText(snapshot_hsv, text_string, position,\n",
    "        font, font_size, font_color, font_thickness)\n",
    "\n",
    "#display the image\n",
    "display_image = cv2.imencode('.png', snapshot_hsv)[1].tostring()\n",
    "widgets.Image(value=display_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the video that you took in the first part of this tutorial and convert it to gray. You can choose to save it as a file or to display the gray frames directly. Either way show your result by displaying it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = cv2.VideoCapture(\"capture.webm\")\n",
    "\n",
    "img_display = widgets.Image()\n",
    "display(img_display)\n",
    "\n",
    "\n",
    "ret, frame = foo.read()\n",
    "while ret:\n",
    "    ret, frame = foo.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "    display_image = cv2.imencode('.png', frame_gray)[1].tostring()\n",
    "    img_display.value = display_image\n",
    "    \n",
    "foo.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record a video that is at least 10 seconds long. Then, use this video and place a label in the top right corner denoting that it is in BGR format. After 5 seconds convert the remaining video into HSV and change the label to indicate that the video is now in HSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "foo = cv2.VideoCapture(\"capture.webm\")\n",
    "\n",
    "img_display = widgets.Image()\n",
    "display(img_display)\n",
    "\n",
    "ret, frame = foo.read()\n",
    "counter = 0\n",
    "while ret:\n",
    "    ret, frame = foo.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    if counter <= 24 * 5:\n",
    "        cv2.putText(frame, \"BGR\", (5, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2)\n",
    "    else:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        cv2.putText(frame, \"HSV\", (5, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2)\n",
    "  \n",
    "    display_image = cv2.imencode('.png', frame)[1].tostring()\n",
    "    img_display.value = display_image\n",
    "    counter += 1\n",
    "\n",
    "foo.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a piece of code that loads the video you recorded in the first part of the tutorial and, for each frame, converts it into both a gray image and a HSV image. Then, stack the images in order HSV - RGB - gray and write them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from IPython.display import display\n",
    "\n",
    "foo = cv2.VideoCapture(\"capture.webm\")\n",
    "writer = cv2.VideoWriter(\"exercise7.mp4\", cv2.CAP_ANY, cv2.VideoWriter_fourcc(*\"H264\"), 24, (640*3,480))\n",
    "\n",
    "img_display = widgets.Image()\n",
    "display(img_display)\n",
    "\n",
    "\n",
    "ret, frame = foo.read()\n",
    "while ret:\n",
    "    ret, frame = foo.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame_hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    frame_gray_img = np.stack([frame_gray] * 3, axis = 2)\n",
    "    frame_stacked = np.hstack([frame_hsv, frame, frame_gray_img])\n",
    "    \n",
    "    display_image = cv2.imencode('.png', frame_stacked)[1].tostring()\n",
    "    img_display.value = display_image\n",
    "    \n",
    "    writer.write(frame_stacked)\n",
    "foo.release()\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.Video.from_file(\"exercise7.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to perform Color Thresholding  to detect the position of Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you will have to record an image that contains an object we want to detect. For this, grab an object that has a solid color, such as a pen, or a banana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CameraStream(constraints=\n",
    "                      {'facing_mode': 'user',\n",
    "                       'audio': False,\n",
    "                       'video': { 'width': 640, 'height': 480 }\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recorder = ImageRecorder(stream=camera)\n",
    "recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = recorder.image.value\n",
    "snapshot = np.frombuffer(snapshot, dtype=np.uint8)\n",
    "snapshot = cv2.imdecode(snapshot, cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `inRange` you can filter an interval for each color channel. For each pixel the command checks \n",
    "```\n",
    "lower < pixel < upper\n",
    "```\n",
    "and creates a mask. The result is a 2-dimensional tensor (matrix) in which a value is 1, if all channels satisfy above condition, or 0 if at least one of the color values of the pixel lies outside the specified range. You can then use this mask to remove parts of the image that you are not interested in, i.e., set them to black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = widgets.Image()\n",
    "\n",
    "def threshold(img, lower, upper):\n",
    "    mask = cv2.inRange(img, np.array(lower), np.array(upper))\n",
    "    masked = cv2.bitwise_and(img, img, mask = mask)\n",
    "        \n",
    "    display_image = cv2.imencode('.png', masked)[1].tostring()\n",
    "    disp.value = display_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore this, you can use Jupyter's interact function. It allows you to specify the upper/lower values of the threshold dynamically using sliders. Try moving them around and see if you can isolate the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "def foo(lower_r, lower_g, lower_b, upper_r, upper_g, upper_b):\n",
    "    lower = [lower_b, lower_g, lower_r]\n",
    "    upper = [upper_b, upper_g, upper_r]\n",
    "    threshold(snapshot, lower, upper)\n",
    "    display(lower, upper)\n",
    "\n",
    "\n",
    "widgets.interact(foo, \n",
    "                 lower_r=IntSlider(min=0, max=255, step=1, value=0), \n",
    "                 lower_g=IntSlider(min=0, max=255, step=1, value=0),\n",
    "                 lower_b=IntSlider(min=0, max=255, step=1, value=0),\n",
    "                 upper_r=IntSlider(min=0, max=255, step=1, value=255),\n",
    "                 upper_g=IntSlider(min=0, max=255, step=1, value=255), \n",
    "                 upper_b=IntSlider(min=0, max=255, step=1, value=255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(disp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is already quite potent, you may have noticed that the threshold values are quite sensitive to changes in illumination. You can test this by shining a light, e.g., your phone's flash, onto the object and taking a picture. This should create a nice color gradient and you will see the problem.\n",
    "\n",
    "To make the detection more robost, it is common to use the HSV colorspace that we saw above. Because color is projected onto a different dimension than illumination (roughly), changes in illumination don't affect the Hue (color) channel as much. Hence, even if you have a color gradient on the object, you should still be able to detect it entirely, if the gradient isn't too stark.\n",
    "\n",
    "Let's look at the difference in code, and explore it similar to what we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp2 = widgets.Image()\n",
    "\n",
    "def thresholdHSV(img, lower, upper):\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)    \n",
    "    mask = cv2.inRange(img_hsv, np.array(lower), np.array(upper))\n",
    "    masked = cv2.bitwise_and(img, img, mask = mask)\n",
    "    \n",
    "    display_image = cv2.imencode('.png', masked)[1].tostring()\n",
    "    disp2.value = display_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "def foo2(lower_H, lower_S, lower_V, upper_H, upper_S, upper_V):\n",
    "    lower = [lower_H, lower_S, lower_V]\n",
    "    upper = [upper_H, upper_S, upper_V]\n",
    "    thresholdHSV(snapshot, lower, upper)\n",
    "    display(lower, upper)\n",
    "\n",
    "widgets.interact(foo2, \n",
    "                 lower_H=IntSlider(min=0, max=255, step=1, value=0), \n",
    "                 lower_S=IntSlider(min=0, max=255, step=1, value=0),\n",
    "                 lower_V=IntSlider(min=0, max=255, step=1, value=0),\n",
    "                 upper_H=IntSlider(min=0, max=255, step=1, value=255),\n",
    "                 upper_S=IntSlider(min=0, max=255, step=1, value=255), \n",
    "                 upper_V=IntSlider(min=0, max=255, step=1, value=255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting the pixels that correspond to an object is nice; however, we usualy want to do more then just know which pixels might be part of the object. This is where contours come in handy. \n",
    "\n",
    "A contour is a line that moves along pixels of equal intensity (same value). In colored images it can be hard to specify what equal value means, so one typically considers grayscale images for this type of analysis. In the case of a binary image like the mask, contour analysis performs similar to connected component analysis. `findContours` is the command that will perform this analysis for you, and the usage is shown below.\n",
    "\n",
    "After you computed the contours in the image, you may want to know where the detected shape is located at. For this you can compute the [image moments](https://en.wikipedia.org/wiki/Image_moment) of the detected shape. Moment is a term that comes from mechanical engineering, and is a measure of inertia of an object under force. We assume that the shape we detected is an object that has a certain amount of mass loacted at each pixel and from that compute the different moments an object of that shape and mass distribution would have. With this trick, it is possible to compute the center of mass of an object (as well as area, orientation, ...).\n",
    "\n",
    "In the example below we compute the center of mass, and then put a dot there and label it as center. You can play around with the image again, and see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp2 = widgets.Image()\n",
    "\n",
    "def thresholdHSV(img, lower, upper):\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)    \n",
    "    mask = cv2.inRange(img_hsv, np.array(lower), np.array(upper))\n",
    "    masked = cv2.bitwise_and(img, img, mask = mask)\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    cv2.drawContours(masked, contours, -1, (0, 0, 255), 2)\n",
    "    \n",
    "    for c in contours:\n",
    "        # compute the center of the contour\n",
    "        M = cv2.moments(c)\n",
    "        if M[\"m00\"] == 0:\n",
    "            continue\n",
    "\n",
    "        cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "        cv2.circle(masked, (cX, cY), 7, (255, 255, 255), -1)\n",
    "        cv2.putText(masked, \"center\", (cX - 20, cY - 20),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    \n",
    "    display_image = cv2.imencode('.png', masked)[1].tostring()\n",
    "    disp2.value = display_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "def foo2(lower_H, lower_S, lower_V, upper_H, upper_S, upper_V):\n",
    "    lower = [lower_H, lower_S, lower_V]\n",
    "    upper = [upper_H, upper_S, upper_V]\n",
    "    thresholdHSV(snapshot, lower, upper)\n",
    "    display(lower, upper)\n",
    "\n",
    "widgets.interact(foo2, \n",
    "                 lower_H=IntSlider(min=0, max=255, step=1, value=0), \n",
    "                 lower_S=IntSlider(min=0, max=255, step=1, value=0),\n",
    "                 lower_V=IntSlider(min=0, max=255, step=1, value=0),\n",
    "                 upper_H=IntSlider(min=0, max=255, step=1, value=255),\n",
    "                 upper_S=IntSlider(min=0, max=255, step=1, value=255), \n",
    "                 upper_V=IntSlider(min=0, max=255, step=1, value=255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the algorithm picks up some artifacts and treats them as objects, too. How to solve this depends on the concrete scenario. In this case we can assume that artifacts are small with respect to the desired object, and hence all but the largest object can be discarded,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp2 = widgets.Image()\n",
    "\n",
    "def thresholdHSV(img, lower, upper):\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)    \n",
    "    mask = cv2.inRange(img_hsv, np.array(lower), np.array(upper))\n",
    "    masked = img.copy() #cv2.bitwise_and(img, img, mask = mask)\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    max_idx = 0\n",
    "    max_val = 0\n",
    "    for idx, c in enumerate(contours):\n",
    "        if cv2.contourArea(c) > max_val:\n",
    "            max_idx = idx\n",
    "            max_val = cv2.contourArea(c)\n",
    "\n",
    "    cv2.drawContours(masked, contours[max_idx], -1, (0, 0, 255), 2)\n",
    "    M = cv2.moments(contours[max_idx])\n",
    "    cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "    cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "    cv2.circle(masked, (cX, cY), 7, (255, 255, 255), -1)\n",
    "    cv2.putText(masked, \"center\", (cX - 20, cY - 20),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    \n",
    "    display_image = cv2.imencode('.png', masked)[1].tostring()\n",
    "    disp2.value = display_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "def foo2(lower_H, lower_S, lower_V, upper_H, upper_S, upper_V):\n",
    "    lower = [lower_H, lower_S, lower_V]\n",
    "    upper = [upper_H, upper_S, upper_V]\n",
    "    thresholdHSV(snapshot, lower, upper)\n",
    "    display(lower, upper)\n",
    "\n",
    "widgets.interact(foo2, \n",
    "                 lower_H=IntSlider(min=0, max=255, step=1, value=0), \n",
    "                 lower_S=IntSlider(min=0, max=255, step=1, value=0),\n",
    "                 lower_V=IntSlider(min=0, max=255, step=1, value=0),\n",
    "                 upper_H=IntSlider(min=0, max=255, step=1, value=255),\n",
    "                 upper_S=IntSlider(min=0, max=255, step=1, value=255), \n",
    "                 upper_V=IntSlider(min=0, max=255, step=1, value=255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record a video where you move the object you used for the color thresholding around in the scene. Then, using the threshold values you estimated when experimenting with above sliders, write a snippet that will read in a video file, and perform the following steps:\n",
    "\n",
    "1. convert the video into HSV\n",
    "3. performs color thresholding on the HSV images\n",
    "4. estimates the position of the object based on the HSV image\n",
    "5. displays a circle at the estimated center of mass of the object\n",
    "6. stacks the original image and the thresholded image horizontally\n",
    "7. labels each image\n",
    "8. creates a video of the frames\n",
    "\n",
    "Your output should look somewhat similar to the example video below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CameraStream(constraints={'facing_mode': 'user', 'audio': False, 'video': {'width': 640, 'height': 480}})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "camera = CameraStream(constraints=\n",
    "                      {'facing_mode': 'user',\n",
    "                       'audio': False,\n",
    "                       'video': { 'width': 640, 'height': 480 }\n",
    "                       })\n",
    "camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26adb23834fa41ad95a760d1420f044a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VideoRecorder(stream=CameraStream(constraints={'facing_mode': 'user', 'audio': False, 'video': {'width': 640, â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recorder2 = VideoRecorder(stream=camera)\n",
    "recorder2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('color_threshold.webm', 'wb') as out_file:\n",
    "    out_file.write(recorder2.video.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-c2a0d7887ff7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# compute the center of the biggest contour\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrawContours\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasked_hsv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontours\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontours\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mcX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"m10\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"m00\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "foo = cv2.VideoCapture(\"color_threshold.webm\")\n",
    "writer = cv2.VideoWriter(\"exercise8.mp4\", cv2.CAP_ANY, cv2.VideoWriter_fourcc(*\"X264\"), 12, (2*640,480))\n",
    "\n",
    "lower_hsv = [15, 67, 95]\n",
    "upper_hsv = [33, 200, 229]\n",
    "\n",
    "lower_rgb = [139, 52, 100]\n",
    "upper_rgb = [206, 114, 134]\n",
    "\n",
    "\n",
    "ret, frame = foo.read()\n",
    "while ret:\n",
    "    ret, frame = foo.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # color threshold using HSV\n",
    "    frame_hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)   \n",
    "    mask = cv2.inRange(frame_hsv, np.array(lower_hsv), np.array(upper_hsv))\n",
    "    masked_hsv = cv2.bitwise_and(frame, frame, mask = mask)\n",
    "    \n",
    "    # draw contours from HSV thresholding\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    max_idx = 0\n",
    "    max_val = 0\n",
    "    for idx, c in enumerate(contours):\n",
    "        if cv2.contourArea(c) > max_val:\n",
    "            max_idx = idx\n",
    "            max_val = cv2.contourArea(c)\n",
    "        \n",
    "    # compute the center of the biggest contour\n",
    "    cv2.drawContours(masked_hsv, contours[max_idx], -1, (255, 255, 255), 2)\n",
    "    M = cv2.moments(contours[max_idx])\n",
    "    cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "    cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "    cv2.circle(masked_hsv, (cX, cY), 7, (255, 255, 255), -1)\n",
    "    cv2.putText(masked_hsv, \"center\", (cX - 20, cY - 20),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    cv2.putText(masked_hsv, \"HSV\", (5, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 4)\n",
    "    \n",
    "    \n",
    "    cv2.putText(frame, \"Original\", (5, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 4)\n",
    "    cv2.drawContours(frame, contours[max_idx], -1, (0, 0, 255), 2)\n",
    "    cv2.circle(frame, (cX, cY), 7, (0, 0, 255), -1)\n",
    "    cv2.putText(frame, \"center\", (cX - 20, cY - 20),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "    \n",
    "    frame_stacked = np.hstack([frame, masked_hsv])\n",
    "    writer.write(frame_stacked)\n",
    "foo.release()\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b355fb45c2440faa0d9ffbfc680756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widgets.Video.from_file(\"exercise8.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection in OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final topic of this lab, you will look into face detection using haar cascades; one of the classic methods to detect faces. It is computationally light weight, and often used as the first step in computer vision pipelines that processes faces.\n",
    "\n",
    "Although deep learning is definitely on the rise, and will undoubtably replace haar cascades at some point in the future, there are still some limitations when it comes to running deep networks in real-time (25 FPS and more), or on embedded devices such as your smartphone. Hence it is good to know about both approaches.\n",
    "\n",
    "![Some example Haar Features](https://i.pinimg.com/originals/1d/ce/fc/1dcefc0ea496c458cf73cc6721c055b4.jpg)\n",
    "\n",
    "So what are haar-cascades? A haar cascade is a combination of two things (1) a set of [haar-like features](https://en.wikipedia.org/wiki/Haar-like_feature), and (2) a cascade classifier. A haar feature is a function that computes a real number from an image, and you can see a visualization of some example haar features in the image above. It is computed by summing all pixel values of the original image that are covered by the white region, and subtracting from that all pixels values that are covered by the black region. This results in a single real number, and, if you use a whole bunch of them, you can create a feature vector from an image. A cascade classifier on the other hand is a function that assigns a label to a set of features. It does so in stages, testing more and more features each time, hence the name cascade, and it tries to reject the example as quickly as possible.\n",
    "\n",
    "A haar-cascade detects the position of a face using a sliding window. That is, it takes the input image, chopps it up into many small parts, and classifies each part into either being a face or not being a face. The majority of these chunks will not be faces; this is what makes haar-cascades so fast, because they are able to reject these regions quickly without spending too much computational resources on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OpenCV it is quite easy to do all of this with a single function call: `cv2.CascadeClassifier.detectMultiScale`. You will have to supply it with a set of features to use, and an image to classify. Fortunately, OpenCV provides a sample set of features, and you can find it in the folder where this notebook is located. First, take a picture to detect a face in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CameraStream(constraints=\n",
    "                      {'facing_mode': 'user',\n",
    "                       'audio': False,\n",
    "                       'video': { 'width': 640, 'height': 480 }\n",
    "                       })\n",
    "camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder = ImageRecorder(stream=camera)\n",
    "recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = recorder.image.value\n",
    "snapshot = np.frombuffer(snapshot, dtype=np.uint8)\n",
    "snapshot = cv2.imdecode(snapshot, cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the class which will perform the face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('frontal_face_features.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes as input a gray image and outputs a list of bounding boxes (rectangles) that contain faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_gray = cv2.cvtColor(snapshot, cv2.COLOR_BGR2GRAY)\n",
    "faces = face_cascade.detectMultiScale(snapshot_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that is left is to draw these rectangles onto the image to visualize the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection = snapshot.copy()\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(detection, (x, y), (x+w, y+h), (255, 0, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image = cv2.imencode('.png', detection)[1].tostring()\n",
    "widgets.Image(value=display_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a pice of code that will load a video and that, for each frame, (1) performs face detection, and (2) draws the bounding box of the detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = cv2.VideoCapture(\"capture.webm\")\n",
    "writer = cv2.VideoWriter(\"exercise9.mp4\", cv2.CAP_ANY, cv2.VideoWriter_fourcc(*\"X264\"), 12, (640,480))\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('frontal_face_features.xml')\n",
    "\n",
    "ret, frame = foo.read()\n",
    "while ret:\n",
    "    ret, frame = foo.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Processing\n",
    "    faces = face_cascade.detectMultiScale(frame_gray)\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            \n",
    "    writer.write(frame)\n",
    "\n",
    "foo.release()\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10 (Bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: For this exercise I will only provide the keywords you need to search for instead of providing example snippets for each keyword. You will have to start consulting resources online, e.g, the documentation, sooner or later if you decide to do your own projects, so it is good to start now.*\n",
    "\n",
    "Using the code you have written above, replace the rectangle that visualizes the detection with an image that is shown on top of a face (for example an emoji). For this, you will have to do the following:\n",
    "\n",
    "1. Remove the line that creates the rectangle in the frame\n",
    "2. Load the image that should be displayed on top of a frame before the main loop (Note: You want a .png image to allow for transparent pixels and use the right option of `cv2.imread` to load the alpha channel)\n",
    "3. split the loaded image into the BGR part (normal image) and alpha channel (mask)\n",
    "4. resize the image to fit the region in which a face was detected (using `cv2.resize`)\n",
    "5. perform `alpha blending` between the image and the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CameraStream(constraints=\n",
    "                      {'facing_mode': 'user',\n",
    "                       'audio': False,\n",
    "                       'video': { 'width': 640, 'height': 480 }\n",
    "                       })\n",
    "camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder = ImageRecorder(stream=camera)\n",
    "recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = recorder.image.value\n",
    "snapshot = np.frombuffer(snapshot, dtype=np.uint8)\n",
    "snapshot = cv2.imdecode(snapshot, cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = cv2.VideoCapture(\"capture.webm\")\n",
    "writer = cv2.VideoWriter(\"exercise10.mp4\", cv2.CAP_ANY, cv2.VideoWriter_fourcc(*\"X264\"), 12, (640,480))\n",
    "\n",
    "img = cv2.imread('viking.png',  cv2.IMREAD_UNCHANGED)\n",
    "mask = img[:,:,3]\n",
    "img = img[:,:,:3]\n",
    "\n",
    "mask = mask.astype(float) / 255\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('frontal_face_features.xml')\n",
    "\n",
    "ret, frame = foo.read()\n",
    "while ret:\n",
    "    ret, frame = foo.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Processing\n",
    "    faces = face_cascade.detectMultiScale(frame_gray)\n",
    "    for (x, y, w, h) in faces:\n",
    "        #cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        resized_img = cv2.resize(img, (h,w)).astype(float)\n",
    "        resized_mask = cv2.resize(mask, (h,w))\n",
    "        frame_float = frame[y:y+h, x:x+w].astype(float)\n",
    "        blend = (1-resized_mask[..., None]) * frame_float + resized_mask[..., None] * resized_img\n",
    "        frame[y:y+h, x:x+w] = blend.astype(np.uint8)\n",
    "            \n",
    "    writer.write(frame)\n",
    "\n",
    "foo.release()\n",
    "writer.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
